# OOPSLA 2021 Artifact

## Getting Started Guide

1. [Install Docker](https://docs.docker.com/engine/install/) and start up the 
docker daemon, either 
[manually](https://docs.docker.com/config/daemon/#start-the-daemon-manually)
or through the 
[system utility](https://docs.docker.com/config/daemon/#start-the-daemon-using-operating-system-utilities).

2. Build the docker image (TODO publish image so it can just be downloaded in this step) (this should take 20-30 minutes):

```sh
docker build --tag oopsla21-nader .
```

3. If the image builds successfully, start a docker container like so: 

```sh
docker run -it -p <port>:<port> --cap-add=sys_nice --name artifact oopsla21-nader
```

4. Test that the artifact works by running: 

```sh
python3 ExpDriver.py --all
```

This command should complete in under an hour. We explain what it does in more 
detail in the next section. 

## Step by Step Instructions

All of the following commands should be run from the `~/nader/` directory. 
Running all experiments fully takes almost two days to complete. 
We have therefore implemented a fast path that can run all experiments 
(on fewer libraries and applications) and finishes in under an hour. 
Our driver runs the fast path by default, so to run the full version of experiments, 
run: 

```sh
python3 ExpDriver.py [OPTIONS] --full
```

To run the __fast__ path on _all_ experiments, run: 

```sh
python3 ExpDriver.py --all
```

To run the __full__ path on _all_ experiments, run: 

```sh
python3 ExpDriver.py --all --full
```

### Generating results/plots

To run individual experiments, simply replace `--all` with the flag corresponding 
to the desired experiment, found by running: 

```sh
python3 ExpDriver.py --help

  ...
  --figure1           generate figure 1
  --table1            generate table 1
  --figure59          generate figures 5 and 9
  --figure7table3     generate figure 7 and table 3
  --table4            generate table 4
  --figure8           generate figure 8
  ...
```

To generate Figure 7 and Table 3, for example, run the following: 

```sh
python3 ExpDriver --figure7table3 [--full]
```

Expected running times for all experiments on a 
2.3 GHz Dual-Core Intel Core i5 Macbook Pro
are listed here:  

| | Figure 1 | Table 1 | Figures 5 and 9 | Figure 7 and Table 3 | Table 4 | Figure 8 | Total |
| --- | --- | --- | --- | --- | --- | --- | --- |
| Fast | 20 min | | 40 min | 2 min | | | |
| Full | 7 hrs | | 9 hrs | 20 min | | | |

### Viewing results/plots

Some expected output is in `example-results`; you can compare your generated plots to those as a sanity check. 
The above result/plot generalization produces a series of PDFs that can be copied out of the docker container and 
viewed locally. If the container is currently running, get the container ID by running: 

```sh
docker container ps

CONTAINER ID   IMAGE           COMMAND  CREATED  STATUS   PORTS   NAMES
<container_id> oopsla21-nader  ...      ...      ...              artifact
```

If the container is stopped, get the 
container ID by running: 

```sh
docker container ps -a
```

Copy files locally using the `docker cp` cmd: 

```sh
docker cp <container_id>:/home/oopsla21ae/images/<plot>.pdf <local_dest>
```

Where <plot>.pdf is any of the files listed in the below subsections. 

In general, the figures and tables produced here are analogous to the figures and 
tables presented in the paper. We describe how to interpret results below, but 
also refer reviewers to the paper for more detailed information. 

#### Figure 1
  
Files: 

```sh
figure1_all.pdf
figure1_histogram.pdf
figure1_hurt.pdf
figure1_improved.pdf
figure1_insignificantly_affected.pdf
```

Figure 1 results are generated by benchmarking 7 Rust libraries that are in the 
top 250 most downloaded Rust libraries from `crates.io`. These 7 libraries use 
unchecked indexing direct and also come with their own benchmarking suites. 
We compile two versions of each library, one with unchecked indexing and one 
where unchecked indexing is converted to checked indexing, to effectively measure 
the overhead of _checked_ indexing (i.e. bounds checks). 

We find that overhead of checked indexing is not fixed and is instead highly
variable: `figure1_hurt.pdf` depicts benchmarks where bounds checks have caused 
slowdowns, `figure1_improved.pdf` depicts benchmarks where bounds checks have 
caused speedups, and `figure1_insignificantly_affected.pdf` depicts benchmarks 
where bounds checks have had no observable effect on performance. These results 
are summarized in `figure1_all.pdf` and `figure1_histogram.pdf`. 

#### Table 1

Files: 

Table 1 results are generated by benchmarking two versions of a 
[Rust implementation](https://github.com/dropbox/rust-brotli-decompressor) of the 
brotli decompression algorithm in three different environments. The two versions
of `rust-brotli` are that with unchecked indexing and that where unchecked 
indexing is converted to checked indexing. The three environments are: 

   * A baseline environment: rustc 1.52, compression level = 5
   * A different workload: rustc 1.52, compression level = 11
   * A different compiler: rustc 1.46, compression level = 5

See [this](https://github.com/nataliepopescu/oopsla21-artifact#paper-claims-not-supported-by-artifact) 
section for why our table is missing the `different architecture` column. 

#### Figures 5 and 9

Files: 

```sh
figure5.pdf
figure9.pdf
```

#### Figure 7 and Table 3

Files: 

```sh
figure7.pdf
table3.pdf
```

#### Table 4

Files: 

#### Figure 8

Files: 

```sh
figure8.pdf
```

TODO how to explain {claims} are supported by artifact? Explain how to interpret results
- generated results are analogous to those in the paper

### Paper claims _not_ supported by artifact

1. The "Different Architecture" column in Table 1 is not supported by our artifact because 
the reviewers may not have access to two or more different architectures on which to 
run our experiments. 

1. The last column of Table 3 is also not supported by our artifact because it was 
the result of a manual process. We moved forward with applications that had 
reasonable synthetic profiling workloads, although there is room for a more 
rigorous process of elimination. 
